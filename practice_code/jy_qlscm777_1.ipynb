{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9011940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "import easydict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d27eb",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75313829",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {'exp_num':'0',\n",
    "     \n",
    "     # Path settings\n",
    "     'data_path':'./train/*/*',\n",
    "     'Kfold':5,\n",
    "     'model_path':'results/',\n",
    "     'image_type':'train_1024', \n",
    "\n",
    "     # Model parameter settings\n",
    "     'encoder_name':'regnety_040',\n",
    "     'drop_path_rate':0.2,\n",
    "     \n",
    "     # Training parameter settings\n",
    "     ## Base Parameter\n",
    "     'img_size':224,\n",
    "     'batch_size':32,\n",
    "     'epochs':25,\n",
    "     'optimizer':'Adam',\n",
    "     'initial_lr':5e-6,\n",
    "     'weight_decay':1e-3,\n",
    "\n",
    "     ## Augmentation\n",
    "     'aug_ver':2,\n",
    "\n",
    "     ## Scheduler (OnecycleLR)\n",
    "     'scheduler':'cycle',\n",
    "     'warm_epoch':5,\n",
    "     'max_lr':1e-3,\n",
    "\n",
    "     ### Cosine Annealing\n",
    "     'min_lr':5e-6,\n",
    "     'tmax':145,\n",
    "\n",
    "     ## etc.\n",
    "     'patience':15,\n",
    "     'clipping':None,\n",
    "\n",
    "     # Hardware settings\n",
    "     'amp':True,\n",
    "     'multi_gpu':False,\n",
    "     'logging':False,\n",
    "     'num_workers':4,\n",
    "     'seed':41\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ba532",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e2eeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
    "\n",
    "# Logging\n",
    "def get_root_logger(logger_name='basicsr',\n",
    "                    log_level=logging.INFO,\n",
    "                    log_file=None):\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    # if the logger has been initialized, just return it\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "    if log_file is not None:\n",
    "        file_handler = logging.FileHandler(log_file, 'w')\n",
    "        file_handler.setFormatter(logging.Formatter(format_str))\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fac4f3",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482a79ba",
   "metadata": {},
   "source": [
    "## Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26640cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from os.path import join as opj\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class Train_Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path_list = df['img_path'].values\n",
    "        self.label = df['label'].values \n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Dataset size:{len(self.img_path_list)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_path_list[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "        label = self.label[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "\n",
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.test_img_path = df['img_path'].values\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size:{len(self.test_img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(test_img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_img_path)\n",
    "\n",
    "def get_loader(df, phase: str, batch_size, shuffle,\n",
    "               num_workers, transform):\n",
    "    if phase == 'test':\n",
    "        dataset = Test_dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "    else:\n",
    "        dataset = Train_Dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True,\n",
    "                                 drop_last=False)\n",
    "    return data_loader\n",
    "\n",
    "def get_train_augmentation(img_size, ver):\n",
    "    if ver==1: # for validset\n",
    "        transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "\n",
    "    if ver == 2:\n",
    "        transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomAffine((20)),\n",
    "                transforms.RandomRotation(90),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4add6",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eecc47ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(args.encoder_name, pretrained=True,\n",
    "                                    drop_path_rate=args.drop_path_rate,\n",
    "                                    )\n",
    "        \n",
    "        num_head = self.encoder.head.fc.in_features\n",
    "        self.encoder.head.fc = nn.Linear(num_head, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class Network_test(nn.Module):\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(encoder_name, pretrained=True,\n",
    "                                    drop_path_rate=0,\n",
    "                                    )\n",
    "        \n",
    "        num_head = self.encoder.head.fc.in_features\n",
    "        self.encoder.head.fc = nn.Linear(num_head, 7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b2610",
   "metadata": {},
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4ecc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from os.path import join as opj\n",
    "from tqdm import tqdm\n",
    "from ptflops import get_model_complexity_info\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.cuda.amp import autocast, grad_scaler\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, args, save_path):\n",
    "        '''\n",
    "        args: arguments\n",
    "        save_path: Model 가중치 저장 경로\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Logging\n",
    "        log_file = os.path.join(save_path, 'log.log')\n",
    "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
    "        self.logger.info(args)\n",
    "        # self.logger.info(args.tag)\n",
    "\n",
    "        # Train, Valid Set load\n",
    "        ############################################################################\n",
    "        if args.step == 0 :\n",
    "            df_train = pd.read_csv('data/train.csv')\n",
    "        else :\n",
    "            df_train = pd.read_csv(f'train_{args.step}step.csv')\n",
    "\n",
    "        if args.image_type is not None:\n",
    "            df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('train_imgs', args.image_type))\n",
    "            df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('test_imgs', 'test_1024'))\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=args.Kfold, shuffle=True, random_state=args.seed)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(df_train)), y=df_train['label'])):\n",
    "            df_train.loc[val_idx, 'fold'] = fold\n",
    "        val_idx = list(df_train[df_train['fold'] == int(args.fold)].index)\n",
    "\n",
    "        df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
    "        df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
    "\n",
    "        # Augmentation\n",
    "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
    "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
    "\n",
    "        # TrainLoader\n",
    "        self.train_loader = get_loader(df_train, phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        self.val_loader = get_loader(df_val, phase='train', batch_size=args.batch_size, shuffle=False,\n",
    "                                       num_workers=args.num_workers, transform=self.test_transform)\n",
    "\n",
    "        # Network\n",
    "        self.model = Network(args).to(self.device)\n",
    "        macs, params = get_model_complexity_info(self.model, (3, args.img_size, args.img_size), as_strings=True,\n",
    "                                                 print_per_layer_stat=False, verbose=False)\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "        iter_per_epoch = len(self.train_loader)\n",
    "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
    "\n",
    "        if args.scheduler == 'step':\n",
    "            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=args.milestone, gamma=args.lr_factor, verbose=True)\n",
    "        elif args.scheduler == 'cos':\n",
    "            tmax = args.tmax # half-cycle \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "\n",
    "        if args.multi_gpu:\n",
    "            self.model = nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "        # Train / Validate\n",
    "        best_loss = np.inf\n",
    "        best_acc = 0\n",
    "        best_epoch = 0\n",
    "        early_stopping = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            if args.scheduler == 'cos':\n",
    "                if epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc, train_f1 = self.training(args)\n",
    "\n",
    "            # Model weight in Multi_GPU or Single GPU\n",
    "            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc, val_f1 = self.validate(args, phase='val')\n",
    "\n",
    "            # Save models\n",
    "            if val_loss < best_loss:\n",
    "                early_stopping = 0\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_f1 = val_f1\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'scheduler': self.scheduler.state_dict(),\n",
    "                    }, os.path.join(save_path, 'best_model.pth'))\n",
    "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "\n",
    "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f} | Val F1:{best_f1:.4f}')\n",
    "        end = time.time()\n",
    "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
    "\n",
    "    # Training\n",
    "    def training(self, args):\n",
    "        self.model.train()\n",
    "        train_loss = AvgMeter()\n",
    "        train_acc = 0\n",
    "        preds_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        scaler = grad_scaler.GradScaler()\n",
    "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
    "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            if self.epoch <= args.warm_epoch:\n",
    "                self.warmup_scheduler.step()\n",
    "\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, targets)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                if args.clipping is not None:\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if args.scheduler == 'cycle':\n",
    "                if self.epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Metric\n",
    "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "            targets_list.extend(targets.cpu().detach().numpy())\n",
    "            # log\n",
    "            train_loss.update(loss.item(), n=images.size(0))\n",
    "\n",
    "        train_acc /= len(self.train_loader.dataset)\n",
    "        train_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
    "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f} | F1:{train_f1:.4f}')\n",
    "        return train_loss.avg, train_acc, train_f1\n",
    "            \n",
    "    # Validation or Dev\n",
    "    def validate(self, args, phase='val'):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = AvgMeter()\n",
    "            val_acc = 0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for i, (images, targets) in enumerate(self.val_loader):\n",
    "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "\n",
    "                # Metric\n",
    "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "                preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "                targets_list.extend(targets.cpu().detach().numpy())\n",
    "\n",
    "                # log\n",
    "                val_loss.update(loss.item(), n=images.size(0))\n",
    "            val_acc /= len(self.val_loader.dataset)\n",
    "            val_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "            self.logger.info(f'{phase} Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f} | F1:{val_f1:.4f}')\n",
    "        return val_loss.avg, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092c06b",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57b5253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print('<---- Training Params ---->')\n",
    "    \n",
    "    # Random Seed\n",
    "    seed = args.seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
    "    \n",
    "    # Create model directory\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    Trainer(args, save_path)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c07d7",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f952238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    model = Network_test(encoder_name).to(device)\n",
    "    model.load_state_dict(torch.load(opj(model_path, 'best_model.pth'))['state_dict'])\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d64bbb",
   "metadata": {},
   "source": [
    "## Make pseudo label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bf42ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_5fold(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'regnety_040', test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = (predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "def make_pseudo_df(train_df, test_df, ensemble, step, threshold = 0.9, z_sample = 500): \n",
    "    train_df_copy = train_df.copy()\n",
    "    test_df_copy = test_df.copy()\n",
    "\n",
    "    test_df_copy['label'] = np.nan\n",
    "    pseudo_test_df = test_df_copy.iloc[np.where(ensemble > threshold)[0]].reset_index(drop=True)\n",
    "\n",
    "    train_df_copy = train_df_copy.append(pseudo_test_df, ignore_index=True).reset_index(drop=True) # reset_index\n",
    "    # print(f'Make train_{step}step.csv')\n",
    "    train_df_copy.to_csv(f'../data/train_{step}step.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4c0b3",
   "metadata": {},
   "source": [
    "## Train & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e5635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 19:32:16,850 INFO: {'exp_num': '0', 'data_path': './train/*/*', 'Kfold': 5, 'model_path': 'results/', 'image_type': 'train_1024', 'encoder_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 224, 'batch_size': 32, 'epochs': 25, 'optimizer': 'Lamb', 'initial_lr': 5e-06, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-06, 'tmax': 145, 'patience': 15, 'clipping': None, 'amp': True, 'multi_gpu': False, 'logging': False, 'num_workers': 4, 'seed': 41, 'step': 0, 'fold': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset size:792\n",
      "<---- Training Params ---->\n",
      "Dataset size:2765\n",
      "Dataset size:692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 19:32:17,078 INFO: Loading pretrained weights from Hugging Face hub (timm/regnety_040.ra3_in1k)\n",
      "2023-05-17 19:32:17,314 INFO: [timm/regnety_040.ra3_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2023-05-17 19:32:22,136 INFO: Computational complexity:       3.98 GMac\n",
      "2023-05-17 19:32:22,136 INFO: Number of parameters:           19.57 M \n",
      "  0%|          | 0/87 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "img_size = 288\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "df_test['img_path'] = df_test['img_path'].apply(lambda x:x.replace('test_imgs', 'test_1024'))\n",
    "test_transform = get_train_augmentation(img_size=img_size, ver=1)\n",
    "test_dataset = Test_dataset(df_test, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "start = 0 # first time : Only Trainset\n",
    "steps = 6 # Number of pseudo labeling times \n",
    "for step in range(start, steps+1): \n",
    "    models_path = []\n",
    "    args.step = step\n",
    "    for s_fold in range(5): # 5fold\n",
    "        args.fold = s_fold\n",
    "        args.exp_num = str(s_fold)\n",
    "        save_path = main(args)\n",
    "        models_path.append(save_path)\n",
    "    ensemble = ensemble_5fold(models_path, test_loader, device)\n",
    "    make_pseudo_df(df_train, df_test, ensemble, step+1)\n",
    "\n",
    "# For submission\n",
    "sub.iloc[:, 1] = ensemble.argmax(axis=1)\n",
    "sub.to_csv(f'./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f73d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
